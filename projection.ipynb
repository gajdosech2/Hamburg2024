{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[Open3D INFO] Resetting default logger to print to terminal.\n",
      "[Open3D INFO] Window window_0 created.\n",
      "[Open3D INFO] EGL headless mode enabled.\n",
      "[Open3D INFO] ICE servers: [\"stun:stun.l.google.com:19302\", \"turn:user:password@34.69.27.100:3478\", \"turn:user:password@34.69.27.100:3478?transport=tcp\"]\n",
      "FEngine (64 bits) created at 0x7fb2bc009660 (threading is enabled)\n",
      "[Open3D INFO] Set WEBRTC_STUN_SERVER environment variable add a customized WebRTC STUN server.\n",
      "[Open3D INFO] WebRTC Jupyter handshake mode enabled.\n",
      "EGL(1.5)\n",
      "OpenGL(4.1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8812fd5eafc34e8aba505f841b9450ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WebVisualizer(window_uid='window_0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.58125214 -0.27518124  0.9192265   1.        ]\n",
      "719\n",
      "209\n",
      "978\n",
      "196\n",
      "752\n",
      "585\n",
      "601\n",
      "481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Sending init frames to window_0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[000:000][508832] (stun_port.cc:96): Binding request timed out from 158.195.89.x:52863 (enp4s0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import open3d as o3d\n",
    "from open3d.web_visualizer import draw\n",
    "\n",
    "ROOT_DIR = \"/home/g/gajdosech2/\"\n",
    "\n",
    "os.chdir(ROOT_DIR + \"/Hamburg2024\")\n",
    "\n",
    "\n",
    "def depth_to_3d(u, v, z, K):\n",
    "    \"\"\"Back-project depth image points to 3D coordinates in the depth camera's coordinate system.\"\"\"\n",
    "    z = z * 0.001\n",
    "    x = (u - K[0, 2]) * z / K[0, 0]\n",
    "    y = (v - K[1, 2]) * z / K[1, 1]\n",
    "    return np.array([x, y, z, 1])\n",
    "\n",
    "\n",
    "def project_to_image_plane_with_distortion(X_world, T_world_to_rgb, K_rgb, dist_rgb):\n",
    "    \"\"\"Project a 3D point in world coordinates onto a 2D RGB image, considering distortion.\"\"\"\n",
    "    # Transform the 3D point from world to RGB camera coordinates\n",
    "    X_rgb = T_world_to_rgb @ X_world\n",
    "    \n",
    "    # Convert 3D point to a format acceptable by cv2.projectPoints\n",
    "    X_rgb_3d = np.array([[X_rgb[:3]]], dtype=np.float64)  # Shape (1, 1, 3)\n",
    "    \n",
    "    # Set rotation and translation vectors (identity, since we're already in camera coordinates)\n",
    "    rvec = np.zeros((3, 1), dtype=np.float64)\n",
    "    tvec = np.zeros((3, 1), dtype=np.float64)\n",
    "    \n",
    "    # Apply distortion correction and project the point to the 2D image plane\n",
    "    undistorted_points, _ = cv2.projectPoints(X_rgb_3d, rvec, tvec, K_rgb, dist_rgb)\n",
    "    \n",
    "    # Extract the 2D pixel coordinates\n",
    "    u_rgb = int(undistorted_points[0][0][0])\n",
    "    v_rgb = int(undistorted_points[0][0][1])\n",
    "    \n",
    "    return u_rgb, v_rgb\n",
    "\n",
    "\n",
    "def project_to_image_plane(X_world, T_world_to_rgb, K_rgb):\n",
    "    \"\"\"Project a 3D point in world coordinates onto a 2D RGB image.\"\"\"\n",
    "    X_rgb = T_world_to_rgb @ X_world  # Transform to RGB camera coordinates\n",
    "    x_rgb, y_rgb, z_rgb = X_rgb[:3] / X_rgb[2]  # Normalize by depth\n",
    "    pixel_coords = K_rgb @ np.array([x_rgb, y_rgb, 1])  # Project onto 2D image plane\n",
    "    u_rgb = int(pixel_coords[0])\n",
    "    v_rgb = int(pixel_coords[1])\n",
    "    return u_rgb, v_rgb\n",
    "\n",
    "\n",
    "def get_transformation(transform_data, image_id, sensor_name):\n",
    "    \"\"\"Extract transformation data for a specific sensor from the JSON.\"\"\"\n",
    "    translation = transform_data[image_id][sensor_name][\"transform\"][\"translation\"]\n",
    "    rotation = transform_data[image_id][sensor_name][\"transform\"][\"rotation\"]\n",
    "    \n",
    "    translation_vector = [translation[\"x\"], translation[\"y\"], translation[\"z\"]]\n",
    "    rotation_quaternion = [rotation[\"x\"], rotation[\"y\"], rotation[\"z\"], rotation[\"w\"]]\n",
    "    \n",
    "    rotation = R.from_quat(rotation_quaternion).as_matrix()\n",
    "\n",
    "    matrix = np.zeros((4, 4))\n",
    "    matrix[:3, :3] = rotation\n",
    "    matrix[:3, 3] = translation_vector\n",
    "    matrix[3, 3] = 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def undistort_depth_point(u_d, v_d, K_depth, dist_depth):\n",
    "    \"\"\"Undistort a point in the depth image using the camera matrix and distortion coefficients.\"\"\"\n",
    "    points = np.array([[[u_d, v_d]]], dtype=np.float32)\n",
    "    undistorted_points = cv2.undistortPoints(points, K_depth, dist_depth, None, K_depth)\n",
    "    u_undistorted = undistorted_points[0][0][0]\n",
    "    v_undistorted = undistorted_points[0][0][1]\n",
    "    return u_undistorted, v_undistorted\n",
    "\n",
    "\n",
    "# plus we have parameters from phillip calibration ONLY for left eye and right eye\n",
    "\n",
    "K_depth = np.array([[914.0937, 0, 649.8485], [0, 914.0947, 370.4816], [0, 0, 1]]) # from API\n",
    "K_rgb_top = K_depth\n",
    "K_rgb_left = np.array([[912.7007, 0, 653.6740], [0, 913.1103, 365.5973], [0, 0, 1]]) # from API\n",
    "K_rgb_right = np.array([[909.7750, 0, 648.5572], [0, 909.3660, 386.9201], [0, 0, 1]]) # from API\n",
    "\n",
    "\n",
    "#K_depth = np.array([[964.276709, 0, 629.718065], [0, 968.782545, 393.747072], [0, 0, 1]]) # from OpenCV\n",
    "#K_rgb_top = K_depth\n",
    "#K_rgb_left = np.array([[942.953774, 0, 633.636017], [0, 950.701779, 332.889790], [0, 0, 1]]) # from OpenCV\n",
    "#K_rgb_right = np.array([[940.120909, 0, 623.323483], [0, 944.625570, 354.214456], [0, 0, 1]]) # from OpenCV\n",
    "\n",
    "\n",
    "K_left_eye = np.array([[911.3754, 0, 933.9101], [0, 909.0219, 758.0138], [0, 0, 1]]) # from Phillip for 1920 x 1440\n",
    "K_right_eye = np.array([[909.4356, 0, 965.7616], [0, 909.0565, 711.3480], [0, 0, 1]]) # from Phillip for 1920 x 1440\n",
    "\n",
    "K_left_eye = np.array([[2203.438019 * 0.345, 0, 2039.254585 * 0.345], [0, 2179.741939 * 0.345, 1866.270673 * 0.345], [0, 0, 1]]) # from OpenCV\n",
    "K_right_eye = np.array([[3460.257628 * 0.345, 0, 2085.007429 * 0.345], [0, 3442.339372 * 0.345, 1661.975531 * 0.345], [0, 0, 1]]) # from OpenCV\n",
    "\n",
    "# RealSense API shows ZERO distortion parameters\n",
    "# For left and right eye we have many distortion parameters from Phillip\n",
    "# plus we have 5-value distortions for all cameras from OpenCV\n",
    "\n",
    "D_depth = np.array([0.074713, -0.133798, 0.005981, -0.008687, 0.000000]) # from OpenCV\n",
    "D_rgb_top = D_depth\n",
    "\n",
    "D_rgb_left = np.array([0.106236, -0.152444, -0.012214, -0.010710, 0.000000]) # from OpenCV\n",
    "D_rgb_right = np.array([0.120152, -0.160957, -0.013440, -0.009624, 0.000000]) # from OpenCV\n",
    "\n",
    "D_left_eye = np.array([-0.303791, 0.072763, -0.016969, 0.001995, 0.000000]) # from OpenCV\n",
    "D_right_eye = np.array([-0.861174, 0.696510, -0.016950, -0.013751, 0.000000]) # from OpenCV\n",
    "\n",
    "\n",
    "image_id = 9\n",
    "with open('dataset/scene_1_caps/pose_transform_data.json', 'r') as f:\n",
    "    transform_data = json.load(f)\n",
    "\n",
    "T_world_to_rgb_top = get_transformation(transform_data, f\"image_{image_id}\", \"realsense_head_color_optical_frame\")\n",
    "T_world_to_rgb_left = get_transformation(transform_data, f\"image_{image_id}\", \"realsense_left_color_optical_frame\")\n",
    "T_world_to_eye_left = get_transformation(transform_data, f\"image_{image_id}\", \"left_eye_cam\")\n",
    "T_world_to_eye_right = get_transformation(transform_data, f\"image_{image_id}\", \"right_eye_cam\")\n",
    "\n",
    "# Inverse transformations from world to RGB cameras\n",
    "T_depth_to_world = np.linalg.inv(T_world_to_rgb_top)\n",
    "T_left_to_world = np.linalg.inv(T_world_to_rgb_left)\n",
    "T_left_eye_to_world = np.linalg.inv(T_world_to_eye_left)\n",
    "T_right_eye_to_world = np.linalg.inv(T_world_to_eye_right)\n",
    "\n",
    "depth_image_top = np.load(f'dataset/scene_10_caps/head_depth_img/{image_id}.npy')\n",
    "rgb_image_top = np.asarray(o3d.io.read_image(f\"dataset/scene_10_caps/head_frame_img/{image_id}.png\"))\n",
    "rgb_image_left = np.asarray(o3d.io.read_image(\"dataset/scene_10_caps/left_frame_img.png\"))\n",
    "rgb_eye_left = np.asarray(o3d.io.read_image(f\"dataset/scene_10_caps/left_eye/{image_id}.png\"))\n",
    "rgb_eye_left = cv2.resize(rgb_eye_left, (1920, 1440)) \n",
    "rgb_eye_left = cv2.cvtColor(rgb_eye_left, cv2.COLOR_BGR2RGB)\n",
    "rgb_eye_right = np.asarray(o3d.io.read_image(f\"dataset/scene_10_caps/right_eye/{image_id}.png\"))\n",
    "rgb_eye_right = cv2.resize(rgb_eye_right, (1920, 1440)) \n",
    "rgb_eye_right = cv2.cvtColor(rgb_eye_right, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "depth_points = [(720, 210, depth_image_top[210, 720])]\n",
    "\n",
    "# Loop through the depth points\n",
    "for (u_d, v_d, z_d) in depth_points:\n",
    "    # Step 1: Back-project to 3D using the depth camera intrinsics\n",
    "\n",
    "    # Undistort the depth pixel\n",
    "    u_undistorted, v_undistorted = undistort_depth_point(u_d, v_d, K_depth, D_depth)\n",
    "\n",
    "    # Now back-project using the undistorted depth pixel coordinates\n",
    "    X_d = depth_to_3d(u_d, v_d, z_d, K_depth)\n",
    "\n",
    "    intrinsics = o3d.camera.PinholeCameraIntrinsic(\n",
    "        width=1280, height=720, fx=K_depth[0, 0], fy=K_depth[1, 1], cx=K_depth[0, 2], cy=K_depth[1, 2]\n",
    "    )\n",
    "    depth_image = o3d.geometry.Image(depth_image_top)\n",
    "    pcd = o3d.geometry.PointCloud.create_from_depth_image(\n",
    "         depth_image,\n",
    "         intrinsics,\n",
    "         depth_scale=1000.0, \n",
    "         depth_trunc=3.0,\n",
    "         stride=1\n",
    "     )\n",
    "    axis_gizmo = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])\n",
    "    sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.02)\n",
    "    sphere.translate(X_d[:3])\n",
    "    # o3d.visualization.draw_geometries([pcd, axis_gizmo, sphere])\n",
    "\n",
    "    draw([pcd, axis_gizmo, sphere])\n",
    "\n",
    "    # Step 2: Transform the 3D point from depth camera to world coordinates\n",
    "    X_world = T_depth_to_world @ X_d \n",
    "\n",
    "    print(X_world)\n",
    "\n",
    "    # Step 3: Project the point to each of the RGB cameras\n",
    "    u_rgb1, v_rgb1 = project_to_image_plane(X_world, T_world_to_rgb_top, K_rgb_top)\n",
    "\n",
    "    print(u_rgb1)\n",
    "    print(v_rgb1)\n",
    "\n",
    "    u_rgb2, v_rgb2 = project_to_image_plane(X_world, T_world_to_rgb_left, K_rgb_left)\n",
    "\n",
    "    print(u_rgb2)\n",
    "    print(v_rgb2)\n",
    "\n",
    "    u_rgb3, v_rgb3 = project_to_image_plane(X_world, T_world_to_eye_left, K_left_eye)\n",
    "\n",
    "    print(u_rgb3)\n",
    "    print(v_rgb3)\n",
    "\n",
    "    u_rgb4, v_rgb4 = project_to_image_plane(X_world, T_world_to_eye_right, K_right_eye)\n",
    "\n",
    "    print(u_rgb4)\n",
    "    print(v_rgb4)\n",
    "\n",
    "    # Step 4: Visualize the point in each of the RGB images\n",
    "    cv2.circle(depth_image_top, (u_d, v_d), 5, (0, 255, 0), -1)\n",
    "    cv2.circle(rgb_image_top, (u_rgb1, v_rgb1), 5, (0, 255, 0), -1)\n",
    "    cv2.circle(rgb_image_left, (u_rgb2, v_rgb2), 5, (0, 255, 0), -1)\n",
    "    cv2.circle(rgb_eye_left, (u_rgb3, v_rgb3), 5, (0, 255, 0), -1)\n",
    "    cv2.circle(rgb_eye_right, (u_rgb4, v_rgb4), 5, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "# Show the results\n",
    "#cv2.imshow(\"Img1\", rgb_image_top)\n",
    "cv2.imwrite(\"work_dirs/Img1.png\", rgb_image_top)\n",
    "#cv2.imshow(\"Img2\", rgb_image_left)\n",
    "cv2.imwrite(\"work_dirs/Img2.png\", rgb_image_left)\n",
    "#cv2.imshow(\"Img3\", rgb_eye_left)\n",
    "#cv2.imwrite(\"Img3.png\", rgb_eye_left)\n",
    "#cv2.imshow(\"Img4\", rgb_eye_right)\n",
    "#cv2.imwrite(\"Img4.png\", rgb_eye_right)\n",
    "#cv2.imshow(\"Img5\", depth_image_top * 255)\n",
    "cv2.imwrite(\"work_dirs/Img5.png\", depth_image_top * 255)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_508729/3444331238.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  u = np.round((fx * points_2d[:, 0] / points_2d[:, 2]) + cx).astype(int)\n",
      "/tmp/ipykernel_508729/3444331238.py:56: RuntimeWarning: invalid value encountered in cast\n",
      "  u = np.round((fx * points_2d[:, 0] / points_2d[:, 2]) + cx).astype(int)\n",
      "/tmp/ipykernel_508729/3444331238.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  v = np.round((fy * points_2d[:, 1] / points_2d[:, 2]) + cy).astype(int)\n",
      "/tmp/ipykernel_508729/3444331238.py:57: RuntimeWarning: invalid value encountered in cast\n",
      "  v = np.round((fy * points_2d[:, 1] / points_2d[:, 2]) + cy).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "get_transformation\n",
      "depth_to_pointcloud\n",
      "apply_transformation\n",
      "estimate_normals\n",
      "create_from_point_cloud_poisson\n",
      "[Open3D INFO] Window window_4 created.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9032e8e873b450fb85e171f85f8676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WebVisualizer(window_uid='window_4')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Sending init frames to window_4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1025:132][508832] (stun_port.cc:96): Binding request timed out from 158.195.89.x:53989 (enp4s0)\n",
      "[1059:386][508832] (stun_port.cc:96): Binding request timed out from 158.195.89.x:37001 (enp4s0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from open3d.web_visualizer import draw\n",
    "\n",
    "ROOT_DIR = \"/home/g/gajdosech2/\"\n",
    "os.chdir(ROOT_DIR + \"/Hamburg2024\")\n",
    "\n",
    "\n",
    "def create_point_cloud_from_depth_and_rgb(depth_image, rgb_image, intrinsics):\n",
    "    camera_matrix = np.array([[intrinsics[\"fx\"], 0, intrinsics[\"cx\"]],\n",
    "                            [0, intrinsics[\"fy\"], intrinsics[\"cy\"]],\n",
    "                            [0, 0, 1]])\n",
    "\n",
    "    intrinsics = o3d.camera.PinholeCameraIntrinsic(\n",
    "        width=1280, height=720, fx=intrinsics[\"fx\"], fy=intrinsics[\"fy\"], cx=intrinsics[\"cx\"], cy=intrinsics[\"cy\"]\n",
    "    )\n",
    "\n",
    "    h, w = depth_image.shape\n",
    "    \n",
    "    # Generate pixel grid\n",
    "    i, j = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    pixels = np.stack([i, j], axis=-1).reshape(-1, 2).astype(np.float32)\n",
    "\n",
    "    # Undistort the pixel coordinates\n",
    "    undistorted_pixels = cv2.undistortPoints(pixels, camera_matrix, D, None, camera_matrix).reshape(h, w, 2)\n",
    "    \n",
    "    # Convert undistorted pixels to normalized coordinates\n",
    "    x_normalized = (undistorted_pixels[..., 0] - camera_matrix[0, 2]) / camera_matrix[0, 0]\n",
    "    y_normalized = (undistorted_pixels[..., 1] - camera_matrix[1, 2]) / camera_matrix[1, 1]\n",
    "    \n",
    "    depth_image[depth_image > 1000] = 0\n",
    "    # Convert to 3D points\n",
    "    z = depth_image * 0.001\n",
    "    x = x_normalized * z\n",
    "    y = y_normalized * z\n",
    "    \n",
    "    # Stack into an Nx3 point cloud\n",
    "    point_cloud = np.stack([x, y, z], axis=-1).reshape(-1, 3)\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "\n",
    "    # Get pixel coordinates for each 3D point\n",
    "    points_2d = np.asarray(pcd.points)\n",
    "\n",
    "    # Project points back to image coordinates\n",
    "    width, height = rgb_image.shape[1], rgb_image.shape[0]\n",
    "    fx, fy = intrinsics.intrinsic_matrix[0, 0], intrinsics.intrinsic_matrix[1, 1]\n",
    "    cx, cy = intrinsics.intrinsic_matrix[0, 2], intrinsics.intrinsic_matrix[1, 2]\n",
    "\n",
    "    # Convert 3D point cloud to 2D pixel coordinates\n",
    "    u = np.round((fx * points_2d[:, 0] / points_2d[:, 2]) + cx).astype(int)\n",
    "    v = np.round((fy * points_2d[:, 1] / points_2d[:, 2]) + cy).astype(int)\n",
    "\n",
    "    # Filter valid points within image bounds\n",
    "    valid_mask = (u >= 0) & (u < width) & (v >= 0) & (v < height)\n",
    "    u, v = u[valid_mask], v[valid_mask]\n",
    "    points_2d = points_2d[valid_mask]\n",
    "\n",
    "    # Get the corresponding RGB values for each valid point\n",
    "    colors = rgb_image[v, u] / 255.0  # Normalize RGB values\n",
    "\n",
    "    # Assign colors to the point cloud\n",
    "    pcd.points = o3d.utility.Vector3dVector(points_2d)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    return pcd\n",
    "\n",
    "def apply_transformation(pcd, translation, rotation):\n",
    "    \"\"\"Apply inverse transformation to point cloud.\"\"\"\n",
    "    rotation = R.from_quat(rotation).as_matrix()\n",
    "\n",
    "    matrix = np.zeros((4, 4))\n",
    "    matrix[:3, :3] = rotation\n",
    "    matrix[:3, 3] = translation\n",
    "    matrix[3, 3] = 1\n",
    "\n",
    "    return pcd.transform(np.linalg.inv(matrix))\n",
    "\n",
    "def get_transformation_data(transform_data, image_id, sensor_name):\n",
    "    \"\"\"Extract transformation data for a specific sensor from the JSON.\"\"\"\n",
    "    translation = transform_data[image_id][sensor_name][\"transform\"][\"translation\"]\n",
    "    rotation = transform_data[image_id][sensor_name][\"transform\"][\"rotation\"]\n",
    "    \n",
    "    translation_vector = [translation[\"x\"], translation[\"y\"], translation[\"z\"]]\n",
    "    rotation_quaternion = [rotation[\"x\"], rotation[\"y\"], rotation[\"z\"], rotation[\"w\"]]\n",
    "    \n",
    "    return translation_vector, rotation_quaternion\n",
    "\n",
    "\n",
    "scene = \"scene_1_chalk\"\n",
    "\n",
    "with open(f'dataset/{scene}/pose_transform_data.json', 'r') as f:\n",
    "    transform_data = json.load(f)\n",
    "\n",
    "\n",
    "D = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "intrinsics_left = {\"fx\": 912.7007, \"fy\": 913.1103, \"cx\": 653.6740, \"cy\": 365.5973}\n",
    "intrinsics_right = {\"fx\": 909.7750, \"fy\": 909.3660, \"cx\": 648.5572, \"cy\": 386.9201}\n",
    "intrinsics_top = {\"fx\": 964.276709, \"fy\": 968.782545, \"cx\": 629.718065, \"cy\": 393.747072}\n",
    "\n",
    "image_id = 0\n",
    "pcds = []\n",
    "\n",
    "for image_id in range(25):\n",
    "    if image_id == 0:\n",
    "        depth_image_right = np.load(f'dataset/{scene}/right_depth_img.npy')\n",
    "        depth_image_left = np.load(f'dataset/{scene}/left_depth_img.npy')\n",
    "\n",
    "        rgb_image_right = np.asarray(o3d.io.read_image(f\"dataset/{scene}/right_frame_img.png\"))\n",
    "        rgb_image_left = np.asarray(o3d.io.read_image(f\"dataset/{scene}/left_frame_img.png\"))\n",
    "\n",
    "        translation_right, rotation_right = get_transformation_data(transform_data, f\"image_{image_id}\", \"realsense_right_color_optical_frame\")\n",
    "        translation_left, rotation_left = get_transformation_data(transform_data, f\"image_{image_id}\", \"realsense_left_color_optical_frame\")\n",
    "\n",
    "        pcd_right = create_point_cloud_from_depth_and_rgb(depth_image_right, rgb_image_right, intrinsics_right)\n",
    "        pcd_left = create_point_cloud_from_depth_and_rgb(depth_image_left, rgb_image_left, intrinsics_left)\n",
    "\n",
    "        pcd_right = apply_transformation(pcd_right, translation_right, rotation_right)\n",
    "        pcd_left = apply_transformation(pcd_left, translation_left, rotation_left)\n",
    "\n",
    "        pcds.append(pcd_left)\n",
    "        pcds.append(pcd_right)\n",
    "\n",
    "    depth_image_top = np.load(f'dataset/{scene}/head_depth_img/{image_id}.npy')\n",
    "    rgb_image_top = np.asarray(o3d.io.read_image(f\"dataset/{scene}/head_frame_img/{image_id}.png\"))\n",
    "\n",
    "    print(\"get_transformation\")\n",
    "    translation_top, rotation_top = get_transformation_data(transform_data, f\"image_{image_id}\", \"realsense_head_color_optical_frame\")\n",
    "\n",
    "    print(\"depth_to_pointcloud\")\n",
    "    pcd_top = create_point_cloud_from_depth_and_rgb(depth_image_top, rgb_image_top, intrinsics_top)\n",
    "\n",
    "    print(\"apply_transformation\")\n",
    "    pcd_top = apply_transformation(pcd_top, translation_top, rotation_top)\n",
    "\n",
    "    pcds.append(pcd_top)\n",
    "\n",
    "# Estimate normals for Poisson surface reconstruction\n",
    "print(\"estimate_normals\")\n",
    "#combined_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.05, max_nn=30))\n",
    "\n",
    "# Perform Poisson surface reconstruction\n",
    "print(\"create_from_point_cloud_poisson\")\n",
    "#mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(combined_pcd, depth=9)\n",
    "\n",
    "axis_gizmo = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])\n",
    "\n",
    "# Combine and visualize\n",
    "#o3d.visualization.draw_geometries([pcd_top, pcd_left, pcd_right, axis_gizmo])\n",
    "draw([axis_gizmo, *pcds])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ham24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
